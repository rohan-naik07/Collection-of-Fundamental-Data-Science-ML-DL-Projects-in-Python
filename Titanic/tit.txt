def initializeWeights(x):
    w=np.zeros(x.shape[0]).reshape(-1,1)
    b=0
    weights={"w":w,"b":b}
    return weights

def cost_function(A,Y,m):
    cost=(-1/m)*np.sum(Y*np.log(A)+(1-Y)*(np.log(1-A)))
    
    assert(isinstance(cost, float))
    return cost
    

def sigmoid(Z):
    A=1/(1+np.exp(-Z))
    return A
def gradient_descent(X,w,b,Y,cost,learning_rate=0.005):
    
    m=X.shape[0]
    
    Z=np.dot(w.T,X)+b
    A=sigmoid(Z)
    cost=cost.append(cost_function(A,Y,m))
    
    dZ=A-Y
    dw=(1/m)*np.dot(X,dZ.T)
    db=(1/m)+np.sum(dZ)
    
    w=w-learning_rate*dw
    b=b-learning_rate*db
    
    weights={"w":w,"b":b}
    return weights

def model(X,Y):
    weights=initializeWeights(X)
    m=Y.shape[1]
    cost=[]
    w=weights['w']
    b=weights['b']
    
    for i in range(0,10000):
          updated_weights=gradient_descent(X,w,b,Y,cost)
          w=updated_weights['w']
          b=updated_weights['b']
           
          if(i%1000)==0:
            print("Cost of model after 1000 ieration is")
            for i in cost:
                print(i)
                print("\n")
                    
    return updated_weights 